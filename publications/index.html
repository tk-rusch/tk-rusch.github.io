<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>

  T. Konstantin Rusch  


  | Publications

</title>
<meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />

<!-- Styles -->

<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>✈︎</text></svg>">

<link rel="stylesheet" href="/assets/css/main.css">
<link rel="canonical" href="/publications/">

<!-- JQuery -->
<!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>


<!-- Theming-->






    
<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams'
    }
  };
</script>
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


  </head>

  <body class="fixed-top-nav ">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
      <a class="navbar-brand title font-weight-lighter" href="/">
       <span class="font-weight-bold">T. Konstantin Rusch</span>   
      </a>
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">
              About
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/biography/">
                Biography
                
              </a>
          </li>
          
          
          
          
          
          <li class="nav-item active">
              <a class="nav-link" href="/publications/">
                Publications
                
                <span class="sr-only">(current)</span>
                
              </a>
          </li>
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/talks/">
                Talks
                
              </a>
          </li>
          
          
          
          
          
          
          
          
          
          
          
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      <div class="post">

  <header class="post-header">
    <h1 class="post-title">Publications</h1>
    <p class="post-description"></p>
  </header>

  <article>
    <div class="publications">


  <h2 class="year">2025</h2>
  <ol class="bibliography"><li><div class="row">
	<div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ICLR</abbr>
    
  
  
  <span class="award badge">Oral</span>
  
  </div>

  <div id="linoss" class="col-sm-9">
    
      <div class="title">Oscillatory State-Space Models</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>T. Konstantin Rusch</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Daniela Rus
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>The 13th International Conference on Learning Representations.</em>
      
      </div>
    
    
    <div class="long_price">
        Oral Presentation (top 1.8% of all submitted papers)
      </div>

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
    
    
    
    
      
      <a href="https://openreview.net/pdf?id=GRMfXcAAFh" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
      <a href="https://github.com/tk-rusch/linoss" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>We propose Linear Oscillatory State-Space models (LinOSS) for efficiently learning on long sequences. Inspired by cortical dynamics of biological neural networks, we base our proposed LinOSS model on a system of forced harmonic oscillators. A stable discretization, integrated over time using fast associative parallel scans, yields the proposed state-space model. We prove that LinOSS produces stable dynamics only requiring nonnegative diagonal state matrix. This is in stark contrast to many previous state-space models relying heavily on restrictive parameterizations. Moreover, we rigorously show that LinOSS is universal, i.e., it can approximate any continuous and causal operator mapping between time-varying functions, to desired accuracy. In addition, we show that an implicit-explicit discretization of LinOSS perfectly conserves the symmetry of time reversibility of the underlying dynamics. Together, these properties enable efficient modeling of long-range interactions, while ensuring stable and accurate long-horizon forecasting. Finally, our empirical results, spanning a wide range of time-series tasks from mid-range to very long-range classification and regression, as well as long-horizon forecasting, demonstrate that our proposed LinOSS model consistently outperforms state-of-the-art sequence models. Notably, LinOSS outperforms Mamba by nearly 2x and LRU by 2.5x on a sequence modeling task with sequences of length 50k.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li><div class="row">
	<div class="col-sm-2 abbr">
  
    
    <abbr class="badge">FPI</abbr>
    
  
  
  </div>

  <div id="linost" class="col-sm-9">
    
      <div class="title">Low Stein Discrepancy via Message-Passing Monte Carlo</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Nathan Kirk,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>T. Konstantin Rusch</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Jakob Zech,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Daniela Rus
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>ICLR Workshop on Frontiers in Probabilistic Inference</em>
      
      </div>
    
    
    <div class="long_price">
        
      </div>

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
    
    
    
    
      
      <a href="https://arxiv.org/pdf/2503.21103" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Message-Passing Monte Carlo (MPMC) was recently introduced as a novel low-discrepancy sampling approach leveraging tools from geometric deep learning. While originally designed for generating uniform point sets, we extend this framework to sample from general multivariate probability distributions with known probability density function. Our proposed method, Stein-Message-Passing Monte Carlo (Stein-MPMC), minimizes a kernelized Stein discrepancy, ensuring improved sample quality. Finally, we show that Stein-MPMC outperforms competing methods, such as Stein Variational Gradient Descent and (greedy) Stein Points, by achieving a lower Stein discrepancy.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li><div class="row">
	<div class="col-sm-2 abbr">
  
    
    <abbr class="badge">WRL</abbr>
    
  
  
  </div>

  <div id="mpmc4prm" class="col-sm-9">
    
      <div class="title">Improving Efficiency of Sampling-based Motion Planning via Message-Passing Monte Carlo</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Makram Chahine,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>T. Konstantin Rusch</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Zach J Patterson,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Daniela Rus
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>ICLR Workshop on Robot Learning</em>
      
      </div>
    
    
    <div class="long_price">
        
      </div>

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
    
    
    
    
      
      <a href="https://arxiv.org/pdf/2410.03909" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Sampling-based motion planning methods, while effective in high-dimensional spaces, often suffer from inefficiencies due to irregular sampling distributions, leading to suboptimal exploration of the configuration space. In this paper, we propose an approach that enhances the efficiency of these methods by utilizing low-discrepancy distributions generated through Message-Passing Monte Carlo (MPMC). MPMC leverages Graph Neural Networks (GNNs) to generate point sets that uniformly cover the space, with uniformity assessed using the the $L_p$-discrepancy measure, which quantifies the irregularity of sample distributions. By improving the uniformity of the point sets, our approach significantly reduces computational overhead and the number of samples required for solving motion planning problems. Experimental results demonstrate that our method outperforms traditional sampling techniques in terms of planning efficiency.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li><div class="row">
	<div class="col-sm-2 abbr">
  
    
    <abbr class="badge">MLGenX</abbr>
    
  
  
  <span class="award badge">Spotlight</span>
  
  </div>

  <div id="equi_multi_learn" class="col-sm-9">
    
      <div class="title">Relaxed Equivariance via Multitask Learning</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Ahmed A. Elhag,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>T. Konstantin Rusch</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Francesco Di Giovanni,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Michael M Bronstein
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>ICLR Workshop on Machine Learning for Genomics Explorations</em>
      
      </div>
    
    
    <div class="long_price">
        Spotlight Presentation
      </div>

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
    
    
    
    
      
      <a href="https://arxiv.org/abs/2410.17878" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Incorporating equivariance as an inductive bias into deep learning architectures to take advantage of the data symmetry has been successful in multiple applications, such as chemistry and dynamical systems. In particular, roto-translations are crucial for effectively modeling geometric graphs and molecules, where understanding the 3D structures enhances generalization. However, equivariant models often pose challenges due to their high computational complexity. In this paper, we introduce REMUL, a training procedure for approximating equivariance with multitask learning. We show that unconstrained models (which do not build equivariance into the architecture) can learn approximate symmetries by minimizing an additional simple equivariance loss. By formulating equivariance as a new learning objective, we can control the level of approximate equivariance in the model. Our method achieves competitive performance compared to equivariant baselines while being 10x faster at inference and 2.5x at training.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li></ol>

  <h2 class="year">2024</h2>
  <ol class="bibliography"><li><div class="row">
	<div class="col-sm-2 abbr">
  
    
    <abbr class="badge">PNAS</abbr>
    
  
  
  </div>

  <div id="mpmc24" class="col-sm-9">
    
      <div class="title">Message-Passing Monte Carlo: Generating low-discrepancy point sets via Graph Neural Networks</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>T. Konstantin Rusch</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Nathan Kirk,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Michael M Bronstein,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Christiane Lemieux,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Daniela Rus
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Proceedings of the National Academy of Sciences</em>
      
      </div>
    
    
    <div class="long_price">
        
      </div>

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
    
    
    
    
      
      <a href="https://arxiv.org/pdf/2405.15059" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
      <a href="https://github.com/tk-rusch/MPMC" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Discrepancy is a well-known measure for the irregularity of the distribution of a point set. Point sets with small discrepancy are called low-discrepancy and are known to efficiently fill the space in a uniform manner. Low-discrepancy points play a central role in many problems in science and engineering, including numerical integration, computer vision, machine perception, computer graphics, machine learning, and simulation. In this work, we present the first machine learning approach to generate a new class of low-discrepancy point sets named Message-Passing Monte Carlo (MPMC) points. Motivated by the geometric nature of generating low-discrepancy point sets, we leverage tools from Geometric Deep Learning and base our model on Graph Neural Networks. We further provide an extension of our framework to higher dimensions, which flexibly allows the generation of custom-made points that emphasize the uniformity in specific dimensions that are primarily important for the particular problem at hand. Finally, we demonstrate that our proposed model achieves state-of-the-art performance superior to previous methods by a significant margin. In fact, MPMC points are empirically shown to be either optimal or near-optimal with respect to the discrepancy for low dimension and small number of points, i.e., for which the optimal discrepancy can be determined.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li><div class="row">
	<div class="col-sm-2 abbr">
  
    
    <abbr class="badge">TMLR</abbr>
    
  
  
  </div>

  <div id="di2023does" class="col-sm-9">
    
      <div class="title">How does over-squashing affect the power of GNNs?</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Francesco Di Giovanni,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>T. Konstantin Rusch</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Michael M. Bronstein,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Andreea Deac,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Marc Lackenby,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Siddhartha Mishra,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Petar Veličković
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Transactions on Machine Learning Research</em>
      
      </div>
    
    
    <div class="long_price">
        
      </div>

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
    
    
    
    
      
      <a href="https://arxiv.org/pdf/2306.03589.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Graph Neural Networks (GNNs) are the state-of-the-art model for machine learning on graph-structured data. The most popular class of GNNs operate by exchanging information between adjacent nodes, and are known as Message Passing Neural Networks (MPNNs). Given their widespread use, understanding the expressive power of MPNNs is a key question. However, existing results typically consider settings with uninformative node features. In this paper, we provide a rigorous analysis to determine which function classes of node features can be learned by an MPNN of a given capacity. We do so by measuring the level of pairwise interactions between nodes that MPNNs allow for. This measure provides a novel quantitative characterization of the so-called over-squashing effect, which is observed to occur when a large volume of messages is aggregated into fixed-size vectors. Using our measure, we prove that, to guarantee sufficient communication between pairs of nodes, the capacity of the MPNN must be large enough, depending on properties of the input graph structure, such as commute times. For many relevant scenarios, our analysis results in impossibility statements in practice, showing that over-squashing hinders the expressive power of MPNNs. We validate our theoretical findings through extensive controlled experiments and ablation studies.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li></ol>

  <h2 class="year">2023</h2>
  <ol class="bibliography"><li><div class="row">
	<div class="col-sm-2 abbr">
  
    
    <abbr class="badge">NeurIPS</abbr>
    
  
  
  </div>

  <div id="lanthaler2023neural" class="col-sm-9">
    
      <div class="title">Neural Oscillators are Universal</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Samuel Lanthaler,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>T. Konstantin Rusch</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Siddhartha Mishra
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>The 37th Conference on Neural Information Processing Systems.</em>
      
      </div>
    
    
    <div class="long_price">
        
      </div>

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
    
    
    
    
      
      <a href="https://arxiv.org/pdf/2305.08753.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Coupled oscillators are being increasingly used as the basis of machine learning (ML) architectures, for instance in sequence modeling, graph representation learning and in physical neural networks that are used in analog ML devices. We introduce an abstract class of neural oscillators that encompasses these architectures and prove that neural oscillators are universal, i.e, they can approximate any continuous and casual operator mapping between time-varying functions, to desired accuracy. This universality result provides theoretical justification for the use of oscillator based ML systems. The proof builds on a fundamental result of independent interest, which shows that a combination of forced harmonic oscillators with a nonlinear read-out suffices to approximate the underlying operators.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li><div class="row">
	<div class="col-sm-2 abbr">
  
    
    <abbr class="badge">PhD thesis</abbr>
    
  
  
  </div>

  <div id="thesis" class="col-sm-9">
    
      <div class="title">Physics-inspired Machine Learning</div>
      <div class="author">
        
          
          
          
          
          
          
            
              <em>Rusch, T. Konstantin</em>
            
          
        
      </div>

      <div class="periodical">
      
        <em>PhD thesis</em>
      
      </div>
    
    
    <div class="long_price">
        
      </div>

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
    
    
    
    
      
      <a href="https://www.research-collection.ethz.ch/bitstream/handle/20.500.11850/637340/1/dissertation.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Physics-inspired machine learning can be seen as incorporating structure from physical systems (e.g.,
given by ordinary or partial differential equations) into machine learning methods to obtain models with
better inductive biases. In this thesis, we provide several of the earliest examples of such methods in
the fields of sequence modelling and graph representation learning. We subsequently show that physicsinspired inductive biases can be leveraged to mitigate important and central issues in each particular
field. More concretely, we demonstrate that systems of coupled nonlinear oscillators and Hamiltonian
systems lead to recurrent sequence models that are able to process sequential interactions over long
time scales by mitigating the exploding and vanishing gradients problem. Additionally, we rigorously
prove that neural systems of oscillators are universal approximators for continuous and causal operators.
Moreover, we show that sequence models derived from multiscale dynamical systems not only mitigate
the exploding and vanishing gradients problem (and are thus able to learn long-term dependencies), but
equally importantly yield expressive models for learning on (real-world) multiscale data. We further show
the impact of physics-inspired approaches on graph representation learning. In particular, systems of
graph-coupled nonlinear oscillators denote a powerful framework for learning on graphs that allows for
stacking many graph neural network (GNN) layers on top of each other. Thereby, we prove that these
systems mitigate the oversmoothing issue in GNNs, where node features exponentially converge to the
same constant node vector for increasing number of GNN layers. Finally, we propose to incorporate
multiple rates that are inferred from the underlying graph data into the message-passing framework of
GNNs. Moreover, we leverage the graph gradient modulated through gating functions to obtain multiple
rates that automatically mitigate the oversmoothing issue. We extensively test all proposed methods on a
variety of versatile synthetic and real-world datasets, ranging from image recognition, speech recognition,
natural language processing (NLP), medical applications, and scientific computing for sequence models,
to citation networks, computational chemistry applications, and article and website networks for graph
learning models.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li><div class="row">
	<div class="col-sm-2 abbr">
  
    
    <abbr class="badge">arXiv</abbr>
    
  
  
  </div>

  <div id="os_survey" class="col-sm-9">
    
      <div class="title">A Survey on Oversmoothing in Graph Neural Networks</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>T. Konstantin Rusch</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Michael M. Bronstein,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Siddhartha Mishra
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>arXiv preprint</em>
      
      </div>
    
    
    <div class="long_price">
        
      </div>

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
    
    
    
    
      
      <a href="https://arxiv.org/pdf/2303.10993.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Node features of graph neural networks (GNNs) tend to become more similar with the increase of the network depth. This effect is known as over-smoothing, which we axiomatically define as the exponential convergence of suitable similarity measures on the node features. Our definition unifies previous approaches and gives rise to new quantitative measures of over-smoothing. Moreover, we empirically demonstrate this behavior for several over-smoothing measures on different graphs (small-, medium-, and large-scale). We also review several approaches for mitigating over-smoothing and empirically test their effectiveness on real-world graph datasets. Through illustrative examples, we demonstrate that mitigating over-smoothing is a necessary but not sufficient condition for building deep GNNs that are expressive on a wide range of graph learning tasks. Finally, we extend our definition of over-smoothing to the rapidly emerging field of continuous-time GNNs.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li><div class="row">
	<div class="col-sm-2 abbr">
  
    
    <abbr class="badge">Physics4ML</abbr>
    
  
  
  <span class="award badge">Spotlight</span>
  
  </div>

  <div id="https://doi.org/10.48550/arxiv.2302.03580" class="col-sm-9">
    
      <div class="title">Multi-Scale Message Passing Neural PDE Solvers</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Léonard Equer,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>T. Konstantin Rusch</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Siddhartha Mishra
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>ICLR Workshop on Physics for Machine Learning</em>
      
      </div>
    
    
    <div class="long_price">
        Spotlight Presentation (top 7% of all submitted papers)
      </div>

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
    
    
    
    
      
      <a href="https://arxiv.org/pdf/2302.03580.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>We propose a novel multi-scale message passing neural network algorithm for learning the solutions of time-dependent PDEs. Our algorithm possesses both temporal and spatial multi-scale resolution features by incorporating multi-scale sequence models and graph gating modules in the encoder and processor, respectively. Benchmark numerical experiments are presented to demonstrate that the proposed algorithm outperforms baselines, particularly on a PDE with a range of spatial and temporal scales.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li><div class="row">
	<div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ICLR</abbr>
    
  
  
  </div>

  <div id="rusch2022gradient" class="col-sm-9">
    
      <div class="title">Gradient Gating for Deep Multi-Rate Learning on Graphs</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>T. Konstantin Rusch</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Benjamin P. Chamberlain,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Michael W. Mahoney,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Michael M. Bronstein,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Siddhartha Mishra
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>The 11th International Conference on Learning Representations.</em>
      
      </div>
    
    
    <div class="long_price">
        
      </div>

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
    
    
    
    
      
      <a href="https://arxiv.org/pdf/2210.00513.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
      <a href="https://github.com/tk-rusch/gradientgating" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>We present Gradient Gating (G^2), a novel framework for improving the performance of
Graph Neural Networks (GNNs). Our framework is based on gating the output of GNN
layers with a mechanism for multi-rate flow of message passing information across nodes of the
underlying graph. Local gradients are harnessed to further modulate message passing updates.
Our framework flexibly allows one to use any basic GNN layer as a wrapper around which
the multi-rate gradient gating mechanism is built. We rigorously prove that G^2
alleviates the oversmoothing problem and allows the design of deep GNNs. Empirical results are presented
to demonstrate that the proposed framework achieves state-of-the-art performance on a variety
of graph learning tasks, including on large-scale heterophilic graphs.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li></ol>

  <h2 class="year">2022</h2>
  <ol class="bibliography"><li><div class="row">
	<div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ICML</abbr>
    
  
  
  </div>

  <div id="graphCON" class="col-sm-9">
    
      <div class="title">Graph-Coupled Oscillator Networks</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>T. Konstantin Rusch</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Benjamin P. Chamberlain,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  James Rowbottom,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Siddhartha Mishra,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Michael M. Bronstein
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>The 39th International Conference on Machine Learning.</em>
      
      </div>
    
    
    <div class="long_price">
        
      </div>

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
    
    
    
    
      
      <a href="https://arxiv.org/pdf/2202.02296.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
      <a href="https://github.com/tk-rusch/GraphCON" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>We propose Graph-Coupled Oscillator Networks (GraphCON), a novel framework for deep learning on graphs. It is based on discretizations of a second-order system of ordinary differential equations (ODEs), which model a network of nonlinear forced and damped oscillators, coupled via the adjacency structure of the underlying graph. The flexibility of our framework permits any basic GNN layer (e.g. convolutional or attentional) as the coupling function, from which a multi-layer deep neural network is built up via the dynamics of the proposed ODEs. We relate the oversmoothing problem, commonly encountered in GNNs, to the stability of steady states of the underlying ODE and show that zero-Dirichlet energy steady states are not stable for our proposed ODEs. This demonstrates that the proposed framework mitigates the oversmoothing problem. Finally, we show that our approach offers competitive performance with respect to the state-of-the-art on a variety of graph-based learning tasks.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li><div class="row">
	<div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ICLR</abbr>
    
  
  
  <span class="award badge">Spotlight</span>
  
  </div>

  <div id="lem" class="col-sm-9">
    
      <div class="title">Long Expressive Memory for Sequence Modeling</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>T. Konstantin Rusch</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Siddhartha Mishra,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  N. Benjamin Erichson,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Michael W. Mahoney
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>The 10th International Conference on Learning Representations.</em>
      
      </div>
    
    
    <div class="long_price">
        Spotlight Presentation (top 6% of all submitted papers)
      </div>

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
    
    
    
    
      
      <a href="https://arxiv.org/pdf/2110.04744.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
      <a href="https://github.com/tk-rusch/LEM" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>We propose a novel method called Long Expressive Memory (LEM) for learning long-term sequential dependencies. LEM is gradient-based, it can efficiently process sequential tasks with very long-term dependencies, and it is sufficiently expressive to be able to learn complicated input-output maps. To derive LEM, we consider a system of multiscale ordinary differential equations, as well as a suitable time-discretization of this system. For LEM, we derive rigorous bounds to show the mitigation of the exploding and vanishing gradients problem, a well-known challenge for gradient-based recurrent sequential learning methods. We also prove that LEM can approximate a large class of dynamical systems to high accuracy. Our empirical results, ranging from image and time-series classification through dynamical systems prediction to speech recognition and language modeling, demonstrate that LEM outperforms state-of-the-art recurrent neural networks, gated recurrent units, and long short-term memory models.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li></ol>

  <h2 class="year">2021</h2>
  <ol class="bibliography"><li><div class="row">
	<div class="col-sm-2 abbr">
  
    
    <abbr class="badge">SISC</abbr>
    
  
  
  </div>

  <div id="longo2020higher" class="col-sm-9">
    
      <div class="title">Higher-Order Quasi-Monte Carlo Training of Deep Neural Networks</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Marcello Longo,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Siddhartha Mishra,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>T. Konstantin Rusch</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Christoph Schwab
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>SIAM Journal on Scientific Computing.</em>
      
      </div>
    
    
    <div class="long_price">
        
      </div>

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
    
    
    
    
      
      <a href="https://arxiv.org/pdf/2009.02713.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
      <a href="https://github.com/tk-rusch/DL-HoQMC" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>We present a novel algorithmic approach and an error analysis leveraging Quasi-Monte Carlo points for training deep neural network (DNN) surrogates of Data-to-Observable (DtO) maps in engineering design. Our analysis reveals higher-order consistent, deterministic choices of training points in the input data space for deep and shallow Neural Networks with holomorphic activation functions such as tanh. These novel training points are proved to facilitate higher-order decay (in terms of the number of training samples) of the underlying generalization error, with consistency error bounds that are free from the curse of dimensionality in the input data space, provided that DNN weights in hidden layers satisfy certain summability conditions. We present numerical experiments for DtO maps from elliptic and parabolic PDEs with uncertain inputs that confirm the theoretical analysis.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li><div class="row">
	<div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ICML</abbr>
    
  
  
  </div>

  <div id="pmlr-v139-rusch21a" class="col-sm-9">
    
      <div class="title">UnICORNN: A recurrent model for learning very long time dependencies</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>T. Konstantin Rusch</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Siddhartha Mishra
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>The 38th International Conference on Machine Learning.</em>
      
      </div>
    
    
    <div class="long_price">
        
      </div>

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
    
    
    
    
      
      <a href="https://arxiv.org/pdf/2103.05487.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
      <a href="https://github.com/tk-rusch/unicornn" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>The design of recurrent neural networks (RNNs)
to accurately process sequential inputs with longtime dependencies is very challenging on account
of the exploding and vanishing gradient problem.
To overcome this, we propose a novel RNN architecture which is based on a structure preserving
discretization of a Hamiltonian system of secondorder ordinary differential equations that models
networks of oscillators. The resulting RNN is fast,
invertible (in time), memory efficient and we derive rigorous bounds on the hidden state gradients
to prove the mitigation of the exploding and vanishing gradient problem. A suite of experiments
are presented to demonstrate that the proposed
RNN provides state of the art performance on a
variety of learning tasks with (very) long-time
dependencies.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li><div class="row">
	<div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ICLR</abbr>
    
  
  
  <span class="award badge">Oral</span>
  
  </div>

  <div id="rusch2021coupled" class="col-sm-9">
    
      <div class="title">Coupled Oscillatory Recurrent Neural Network (coRNN): An accurate and (gradient) stable architecture for learning long time dependencies</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>T. Konstantin Rusch</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Siddhartha Mishra
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>The 9th International Conference on Learning Representations.</em>
      
      </div>
    
    
    <div class="long_price">
        Oral Presentation (top 1% of all submitted papers)
      </div>

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
    
    
    
    
      
      <a href="https://openreview.net/pdf?id=F3s69XzWOia" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
      <a href="https://github.com/tk-rusch/coRNN" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Circuits of biological neurons, such as in the functional parts of the brain can be modeled as networks of coupled oscillators. Inspired by the ability of these systems to express a rich set of outputs while keeping (gradients of) state variables bounded, we propose a novel architecture for recurrent neural networks. Our proposed RNN is based on a time-discretization of a system of second-order ordinary differential equations, modeling networks of controlled nonlinear oscillators. We prove precise bounds on the gradients of the hidden states, leading to the mitigation of the exploding and vanishing gradient problem for this RNN. Experiments show that the proposed RNN is comparable in performance to the state of the art on a variety of benchmarks, demonstrating the potential of this architecture to provide stable and accurate RNNs for processing complex sequential data.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li><div class="row">
	<div class="col-sm-2 abbr">
  
    
    <abbr class="badge">SINUM</abbr>
    
  
  
  </div>

  <div id="mishra2021enhancing" class="col-sm-9">
    
      <div class="title">Enhancing accuracy of deep learning algorithms by training with low-discrepancy sequences</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Siddhartha Mishra,
                
              
            
          
        
          
          
          
          
          
          
            
              
                and <em>T. Konstantin Rusch</em>
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>SIAM Journal on Numerical Analysis.</em>
      
      </div>
    
    
    <div class="long_price">
        
      </div>

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
    
    
    
    
      
      <a href="https://arxiv.org/pdf/2005.12564.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
      <a href="https://github.com/tk-rusch/Deep_QMC_sampling" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>We propose a deep supervised learning algorithm based on low-discrepancy sequences as the training
set. By a combination of theoretical arguments and extensive numerical experiments we demonstrate
that the proposed algorithm significantly outperforms standard deep learning algorithms that are
based on randomly chosen training data, for problems in moderately high dimensions. The proposed
algorithm provides an efficient method for building inexpensive surrogates for many underlying maps
in the context of scientific computing.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li></ol>

  <h2 class="year">2018</h2>
  <ol class="bibliography"><li><div class="row">
	<div class="col-sm-2 abbr">
  
    
    <abbr class="badge">Turbo Expo</abbr>
    
  
  
  </div>

  <div id="rusch2018reproducing" class="col-sm-9">
    
      <div class="title">Reproducing Existing Nacelle Geometries With the Free-Form Deformation Parametrization</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>T. Konstantin Rusch</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Martin Siggel,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Richard-Gregor Becker
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>ASME Turbo Expo: Power for Land, Sea, and Air.</em>
      
      </div>
    
    
    <div class="long_price">
        
      </div>

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>In the conceptual and preliminary aircraft design phase the
Free-Form Deformation (FFD) is one of various parametrization
schemes to define the geometry of an engine’s nacelle. This paper
presents a method that is able to create a C2 continuous periodic
approximation of existing reference nacelles with the B-spline
based FFD, which is a generalization of the classical FFD. The
basic principle of this method is to start with a rotational symmetric B-spline approximation of the reference nacelle, which is
subsequently deformed with a FFD grid that is placed around
the initial geometry. A method is derived that computes the displacement of the FFD grid points, such that the deformed nacelle
approximates the reference nacelle with minimal deviations. As
this turns out to be a linear inverse problem, it can be solved with
a linear least squares fit. To avoid overfitting effects – like degenerative FFD grids which imply excessive local deformations
– the inverse problem is regularized with the Tikhonov approach.
The NASA CRM model and the IAE V2500 engine have been selected as reference geometries. Both resemble nacelles that are
typically found on common aircraft models and both deviate sufficiently from the rotational symmetry. It is demonstrated that the
mean error of the approximation decreases with an increase of
the number of FFD grid points and how the regularization affects
these results. Finally, the B-spline based FFD with the classical
Bernstein based FFD are compared for both models. The results conceptually prove the usability of the FFD approach for
the construction of nacelle geometries in the preliminary aircraft
design phase.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li></ol>


</div>

  </article>

</div>

    </div>

    <!-- Footer -->

    
<footer class="fixed-bottom">
  <div class="container mt-0">
    &copy; Copyright 2025 T. Konstantin Rusch  .
    Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme.
    
    
  </div>
</footer>



  </body>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/assets/js/mansory.js" type="text/javascript"></script>


  


<!-- Medium Zoom JS -->
<script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
<script src="/assets/js/zoom.js"></script>


<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>


</html>
